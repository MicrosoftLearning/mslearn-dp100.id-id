{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bekerja dengan Data\n",
        "\n",
        "Data adalah fondasi tempat model pembelajaran mesin dibangun. Mengelola data secara terpusat di cloud, dan membuat data dapat diakses oleh tim ilmuwan data yang menjalankan eksperimen dan model pelatihan di beberapa stasiun kerja dan target komputasi adalah bagian penting dari solusi ilmu data profesional.\n",
        "\n",
        "Di buku catatan ini, Anda akan menjelajahi dua objek Azure Machine Learning untuk bekerja dengan data: *penyimpanan data*, dan *himpunan data*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Menghubungkan ke ruang kerja Anda\n",
        "\n",
        "Untuk memulai, hubungkan ke ruang kerja Anda.\n",
        "\n",
        "> **Catatan**: Jika Anda belum membuat sesi yang terautentikasi dengan langganan Azure, Anda akan diminta untuk mengautentikasi dengan mengklik tautan, memasukkan kode autentikasi, dan masuk ke Azure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366182180
        }
      },
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bekerja dengan penyimpanan data\n",
        "\n",
        "Di Azure Machine Learning, *penyimpanan data* adalah referensi ke lokasi penyimpanan, seperti kontainer blob Azure Storage. Setiap ruang kerja memiliki penyimpanan data default - biasanya kontainer blob penyimpanan Azure yang dibuat dengan ruang kerja. Jika Anda perlu bekerja dengan data yang disimpan di lokasi yang berbeda, Anda dapat menambahkan penyimpanan data kustom ke ruang kerja Anda dan mengatur penyimpanan data kustom ke default.\n",
        "\n",
        "### Melihat penyimpanan data\n",
        "\n",
        "Jalankan kode berikut untuk menentukan penyimpanan data di ruang kerja Anda:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366185140
        }
      },
      "outputs": [],
      "source": [
        "# Get the default datastore\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "# Enumerate all datastores, indicating which is the default\n",
        "for ds_name in ws.datastores:\n",
        "    print(ds_name, \"- Default =\", ds_name == default_ds.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anda juga dapat melihat dan mengelola penyimpanan data di ruang kerja Anda di halaman **Penyimpanan Data** untuk ruang kerja Anda di [Studio Azure Machine Learning](https://ml.azure.com).\n",
        "\n",
        "### Mengunggah data ke penyimpanan data\n",
        "\n",
        "Sekarang setelah Anda menentukan penyimpanan data yang tersedia, Anda dapat mengunggah file dari sistem file lokal ke penyimpanan data sehingga dapat diakses oleh eksperimen yang berjalan di ruang kerja, di mana pun skrip eksperimen sedang dijalankan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366190608
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "\n",
        "Dataset.File.upload_directory(src_dir='data',\n",
        "                              target=DataPath(default_ds, 'diabetes-data/')\n",
        "                              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bekerja dengan himpunan data\n",
        "\n",
        "Azure Machine Learning menyediakan abstraksi untuk data dalam bentuk *himpunan data*. Himpunan data adalah referensi yang memiliki versi ke himpunan data tertentu yang mungkin ingin Anda gunakan dalam eksperimen. Himpunan data dapat berbasis *tabular* atau *file*.\n",
        "\n",
        "### Membuat himpunan data tabular\n",
        "\n",
        "Mari kita buat himpunan data dari data diabetes yang Anda unggah ke penyimpanan data, dan lihat 20 catatan pertama. Dalam kasus ini, data dalam format terstruktur dalam file CSV, jadi kita akan menggunakan himpunan data *tabular*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366193530
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Dataset\n",
        "\n",
        "# Get the default datastore\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "#Create a tabular dataset from the path on the datastore (this may take a short while)\n",
        "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
        "\n",
        "# Display the first 20 rows as a Pandas dataframe\n",
        "tab_data_set.take(20).to_pandas_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seperti yang dapat Anda lihat pada kode di atas, mudah untuk mengonversi himpunan data tabular ke kerangka data Pandas, memungkinkan Anda bekerja dengan data menggunakan teknik python umum.\n",
        "\n",
        "### Membuat Himpunan Data file\n",
        "\n",
        "Himpunan data yang Anda buat adalah himpunan data *tabular* yang dapat dibaca sebagai kerangka data yang berisi semua data dalam file terstruktur yang disertakan dalam definisi himpunan data. Ini berfungsi dengan baik untuk data tabular, tetapi dalam beberapa skenario pembelajaran mesin, Anda mungkin perlu bekerja dengan data yang tidak terstruktur; atau Anda mungkin hanya ingin menangani pembacaan data dari file dalam kode Anda sendiri. Untuk mencapai hal ini, Anda dapat menggunakan himpunan data *file*, yang mencantumkan jalur file di titik pemasangan virtual, yang dapat Anda gunakan untuk membaca data dalam file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366196795
        }
      },
      "outputs": [],
      "source": [
        "#Create a file dataset from the path on the datastore (this may take a short while)\n",
        "file_data_set = Dataset.File.from_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
        "\n",
        "# Get the files in the dataset\n",
        "for file_path in file_data_set.to_path():\n",
        "    print(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mendaftarkan himpunan data\n",
        "\n",
        "Sekarang setelah Anda membuat himpunan data yang mereferensikan data diabetes, Anda dapat mendaftarkannya agar mudah diakses oleh eksperimen apa pun yang dijalankan di ruang kerja.\n",
        "\n",
        "Kita akan mendaftarkan himpunan data tabular sebagai **himpunan data diabetes**, dan himpunan data file sebagai **file diabetes**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366200005
        }
      },
      "outputs": [],
      "source": [
        "# Register the tabular dataset\n",
        "try:\n",
        "    tab_data_set = tab_data_set.register(workspace=ws, \n",
        "                                        name='diabetes dataset',\n",
        "                                        description='diabetes data',\n",
        "                                        tags = {'format':'CSV'},\n",
        "                                        create_new_version=True)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "\n",
        "# Register the file dataset\n",
        "try:\n",
        "    file_data_set = file_data_set.register(workspace=ws,\n",
        "                                            name='diabetes file dataset',\n",
        "                                            description='diabetes files',\n",
        "                                            tags = {'format':'CSV'},\n",
        "                                            create_new_version=True)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "\n",
        "print('Datasets registered')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anda dapat melihat dan mengelola himpunan data di halaman **Himpunan Data** untuk ruang kerja Anda di [Studio Azure Machine Learning](https://ml.azure.com). Anda juga bisa mendapatkan daftar himpunan data dari objek ruang kerja:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366204696
        }
      },
      "outputs": [],
      "source": [
        "print(\"Datasets:\")\n",
        "for dataset_name in list(ws.datasets.keys()):\n",
        "    dataset = Dataset.get_by_name(ws, dataset_name)\n",
        "    print(\"\\t\", dataset.name, 'version', dataset.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kemampuan untuk menetapkan versi himpunan data memungkinkan Anda mendefinisikan ulang himpunan data tanpa merusak eksperimen atau alur yang ada yang mengandalkan definisi sebelumnya. Secara default, versi terbaru dari himpunan data bernama dikembalikan, tetapi Anda dapat mengambil versi tertentu dari himpunan data dengan menentukan nomor versi, seperti ini:\n",
        "\n",
        "```python\n",
        "dataset_v1 = Dataset.get_by_name(ws, 'diabetes dataset', version = 1)\n",
        "```\n",
        "\n",
        "\n",
        "### Melatih model dari himpunan data tabular\n",
        "\n",
        "Sekarang setelah Anda memiliki himpunan data, Anda siap untuk memulai model pelatihan dari himpunan data. Anda dapat meneruskan himpunan data ke skrip sebagai *input* dalam penghitung yang digunakan untuk menjalankan skrip.\n",
        "\n",
        "Jalankan dua sel kode berikut untuk membuat:\n",
        "\n",
        "1. Folder bernama **diabetes_training_from_tab_dataset**\n",
        "2. Skrip yang melatih model klasifikasi dengan menggunakan himpunan data tabular yang diteruskan sebagai argumen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366207914
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create a folder for the experiment files\n",
        "experiment_folder = 'diabetes_training_from_tab_dataset'\n",
        "os.makedirs(experiment_folder, exist_ok=True)\n",
        "print(experiment_folder, 'folder created')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $experiment_folder/diabetes_training.py\n",
        "# Import libraries\n",
        "import os\n",
        "import argparse\n",
        "from azureml.core import Run, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Get the script arguments (regularization rate and training dataset ID)\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01, help='regularization rate')\n",
        "parser.add_argument(\"--input-data\", type=str, dest='training_dataset_id', help='training dataset')\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Set regularization hyperparameter (passed as an argument to the script)\n",
        "reg = args.reg_rate\n",
        "\n",
        "# Get the experiment run context\n",
        "run = Run.get_context()\n",
        "\n",
        "# Get the training dataset\n",
        "print(\"Loading Data...\")\n",
        "diabetes = run.input_datasets['training_data'].to_pandas_dataframe()\n",
        "\n",
        "# Separate features and labels\n",
        "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
        "\n",
        "# Split data into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "# Train a logistic regression model\n",
        "print('Training a logistic regression model with regularization rate of', reg)\n",
        "run.log('Regularization Rate',  np.float(reg))\n",
        "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
        "\n",
        "# calculate accuracy\n",
        "y_hat = model.predict(X_test)\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy:', acc)\n",
        "run.log('Accuracy', np.float(acc))\n",
        "\n",
        "# calculate AUC\n",
        "y_scores = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "print('AUC: ' + str(auc))\n",
        "run.log('AUC', np.float(auc))\n",
        "\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
        "joblib.dump(value=model, filename='outputs/diabetes_model.pkl')\n",
        "\n",
        "run.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Catatan**: Dalam skrip, himpunan data diteruskan sebagai parameter (atau argumen). Dalam kasus himpunan data tabular, argumen ini akan berisi ID dari himpunan data terdaftar; jadi Anda dapat menulis kode dalam skrip untuk mendapatkan ruang kerja eksperimen dari konteks eksekusi, lalu mendapatkan himpunan data menggunakan ID himpunan data; seperti ini:\n",
        ">\n",
        "> ```\n",
        "> run = Run.get_context()\n",
        "> ws = run.experiment.workspace\n",
        "> dataset = Dataset.get_by_id(ws, id=args.training_dataset_id)\n",
        "> diabetes = dataset.to_pandas_dataframe()\n",
        "> ```\n",
        ">\n",
        "> Tetapi, eksekusi Azure Machine Learning mengidentifikasi argumen yang mereferensikan himpunan data bernama secara otomatis dan menambahkannya ke kumpulan **input_datasets** eksekusi, sehingga Anda juga dapat mengambil himpunan data dari koleksi ini dengan menentukan \"nama ramah\" koleksi (yang akan Anda lihat sebentar lagi, ditentukan dalam definisi argumen dalam konfigurasi eksekusi skrip untuk eksperimen). Ini merupakan pendekatan yang diambil dalam skrip di atas.\n",
        "\n",
        "Sekarang Anda dapat menjalankan skrip sebagai eksperimen, menentukan argumen untuk himpunan data pelatihan, yang dibaca oleh skrip.\n",
        "\n",
        "> **Catatan**: Kelas **Himpunan Data** bergantung pada beberapa komponen dalam paket **azureml-dataprep**, jadi Anda harus menyertakan paket ini di lingkungan tempat eksperimen pelatihan akan dijalankan. Paket **azureml-dataprep** disertakan dalam paket **azure-defaults**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366222016
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
        "from azureml.core.runconfig import DockerConfiguration\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "\n",
        "# Create a Python environment for the experiment (from a .yml file)\n",
        "env = Environment.from_conda_specification(\"experiment_env\", \"environment.yml\")\n",
        "\n",
        "# Get the training dataset\n",
        "diabetes_ds = ws.datasets.get(\"diabetes dataset\")\n",
        "\n",
        "# Create a script config\n",
        "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
        "                              script='diabetes_training.py',\n",
        "                              arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n",
        "                                           '--input-data', diabetes_ds.as_named_input('training_data')], # Reference to dataset\n",
        "                              environment=env,\n",
        "                              docker_runtime_config=DockerConfiguration(use_docker=True)) \n",
        "\n",
        "# submit the experiment\n",
        "experiment_name = 'mslearn-train-diabetes'\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\n",
        "run = experiment.submit(config=script_config)\n",
        "RunDetails(run).show()\n",
        "run.wait_for_completion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Catatan:** Argumen **--input-data** meneruskan himpunan data sebagai *input bernama* yang menyertakan *nama ramah* untuk himpunan data, yang digunakan oleh skrip untuk membacanya dari kumpulan **input_datasets** dalam eksperimen yang dijalankan. Nilai string dalam argumen **--input-data** sebenarnya adalah ID himpunan data yang terdaftar.  Sebagai pendekatan alternatif, Anda cukup meneruskan `diabetes_ds.id`, dalam kasus ini skrip dapat mengakses ID himpunan data dari argumen skrip dan menggunakannya untuk mendapatkan himpunan data dari ruang kerja, tetapi tidak dari kumpulan **input_datasets**.\n",
        "\n",
        "Pertama kali eksperimen dijalankan, mungkin diperlukan beberapa waktu untuk menyiapkan lingkungan Python - eksekusi selanjutnya akan lebih cepat.\n",
        "\n",
        "Setelah eksperimen selesai, di widget, lihat log output **azureml-logs/70_driver_log.txt** dan metrik yang dihasilkan oleh eksekusi.\n",
        "\n",
        "### Mendaftarkan model terlatih\n",
        "\n",
        "Seperti eksperimen pelatihan apa pun, Anda dapat mengambil model terlatih dan mendaftarkan model di ruang kerja Azure Machine Learning Anda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366229781
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Model\n",
        "\n",
        "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
        "                   tags={'Training context':'Tabular dataset'}, properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
        "\n",
        "for model in Model.list(ws):\n",
        "    print(model.name, 'version:', model.version)\n",
        "    for tag_name in model.tags:\n",
        "        tag = model.tags[tag_name]\n",
        "        print ('\\t',tag_name, ':', tag)\n",
        "    for prop_name in model.properties:\n",
        "        prop = model.properties[prop_name]\n",
        "        print ('\\t',prop_name, ':', prop)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Melatih model dari himpunan data file\n",
        "\n",
        "Anda telah melihat cara melatih model menggunakan data pelatihan dalam himpunan data *tabular*; tetapi bagaimana dengan himpunan data *file*?\n",
        "\n",
        "Saat Anda menggunakan himpunan data file, argumen himpunan data yang diteruskan ke skrip mewakili titik pemasangan yang berisi jalur file. Cara Anda membaca data dari file ini bergantung pada jenis data dalam file dan apa yang ingin Anda lakukan dengan file. Dalam kasus file CSV diabetes, Anda dapat menggunakan modul **glob** Python untuk membuat daftar file di titik pemasangan virtual yang ditentukan oleh himpunan data, dan membaca semuanya ke dalam kerangka data Pandas yang digabungkan menjadi kerangka data tunggal.\n",
        "\n",
        "Jalankan dua sel kode berikut untuk membuat:\n",
        "\n",
        "1. Folder bernama **diabetes_training_from_file_dataset**\n",
        "2. Skrip yang melatih model klasifikasi dengan menggunakan himpunan data file yang diteruskan adalah sebagai *input*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366232676
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create a folder for the experiment files\n",
        "experiment_folder = 'diabetes_training_from_file_dataset'\n",
        "os.makedirs(experiment_folder, exist_ok=True)\n",
        "print(experiment_folder, 'folder created')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $experiment_folder/diabetes_training.py\n",
        "# Import libraries\n",
        "import os\n",
        "import argparse\n",
        "from azureml.core import Dataset, Run\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "import glob\n",
        "\n",
        "# Get script arguments (rgularization rate and file dataset mount point)\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01, help='regularization rate')\n",
        "parser.add_argument('--input-data', type=str, dest='dataset_folder', help='data mount point')\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Set regularization hyperparameter (passed as an argument to the script)\n",
        "reg = args.reg_rate\n",
        "\n",
        "# Get the experiment run context\n",
        "run = Run.get_context()\n",
        "\n",
        "# load the diabetes dataset\n",
        "print(\"Loading Data...\")\n",
        "data_path = run.input_datasets['training_files'] # Get the training data path from the input\n",
        "# (You could also just use args.dataset_folder if you don't want to rely on a hard-coded friendly name)\n",
        "\n",
        "# Read the files\n",
        "all_files = glob.glob(data_path + \"/*.csv\")\n",
        "diabetes = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
        "\n",
        "# Separate features and labels\n",
        "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
        "\n",
        "# Split data into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "# Train a logistic regression model\n",
        "print('Training a logistic regression model with regularization rate of', reg)\n",
        "run.log('Regularization Rate',  np.float(reg))\n",
        "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
        "\n",
        "# calculate accuracy\n",
        "y_hat = model.predict(X_test)\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy:', acc)\n",
        "run.log('Accuracy', np.float(acc))\n",
        "\n",
        "# calculate AUC\n",
        "y_scores = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "print('AUC: ' + str(auc))\n",
        "run.log('AUC', np.float(auc))\n",
        "\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
        "joblib.dump(value=model, filename='outputs/diabetes_model.pkl')\n",
        "\n",
        "run.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sama seperti himpunan data tabular, Anda dapat mengambil himpunan data file dari kumpulan **input_datasets** dengan menggunakan nama ramah himpunan data file. Anda juga dapat mengambil himpunan data file dari argumen skrip, yang dalam kasus himpunan data file berisi jalur pemasangan ke file (bukan ID himpunan data yang diteruskan untuk himpunan data tabular).\n",
        "\n",
        "Selanjutnya kita perlu mengubah cara kita meneruskan himpunan data ke skrip - kita perlu menentukan jalur tempat skrip dapat membaca file. Anda dapat menggunakan metode **as_download** atau **as_mount** untuk melakukan hal ini. Dengan menggunakan **as_download** akan menyebabkan file dalam himpunan data file diunduh ke lokasi sementara pada komputasi tempat skrip dijalankan, sementara **as_mount** membuat titik pemasangan tempat file dapat dialirkan langsung dari penyimpanan data.\n",
        "\n",
        "Anda dapat menggabungkan metode akses dengan metode **as_named_input** untuk menyertakan himpunan data dalam kumpulan **input_datasets** dalam eksekusi eksperimen (jika Anda menghilangkannya, misalnya dengan mengatur argumen ke `diabetes_ds.as_mount()`, skrip akan dapat mengakses titik pemasangan himpunan data dari argumen skrip, tetapi tidak dari kumpulan **input_datasets**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366252096
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.core.runconfig import DockerConfiguration\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "\n",
        "# Get the training dataset\n",
        "diabetes_ds = ws.datasets.get(\"diabetes file dataset\")\n",
        "\n",
        "# Create a script config\n",
        "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
        "                                script='diabetes_training.py',\n",
        "                                arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n",
        "                                             '--input-data', diabetes_ds.as_named_input('training_files').as_download()], # Reference to dataset location\n",
        "                                environment=env, # Use the environment created previously\n",
        "                                docker_runtime_config=DockerConfiguration(use_docker=True))\n",
        "\n",
        "# submit the experiment\n",
        "experiment_name = 'mslearn-train-diabetes'\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\n",
        "run = experiment.submit(config=script_config)\n",
        "RunDetails(run).show()\n",
        "run.wait_for_completion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setelah eksperimen selesai, di widget, lihat log output **azureml-logs/70_driver_log.txt** untuk memverifikasi bahwa file dalam himpunan data file telah diunduh ke folder sementara guna mengizinkan skrip untuk membaca file.\n",
        "\n",
        "### Mendaftarkan model terlatih\n",
        "\n",
        "Sekali lagi, Anda dapat mendaftarkan model yang dilatih oleh eksperimen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649366260993
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Model\n",
        "\n",
        "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
        "                   tags={'Training context':'File dataset'}, properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
        "\n",
        "for model in Model.list(ws):\n",
        "    print(model.name, 'version:', model.version)\n",
        "    for tag_name in model.tags:\n",
        "        tag = model.tags[tag_name]\n",
        "        print ('\\t',tag_name, ':', tag)\n",
        "    for prop_name in model.properties:\n",
        "        prop = model.properties[prop_name]\n",
        "        print ('\\t',prop_name, ':', prop)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Informasi Selengkapnya**: Untuk informasi selengkapnya tentang pelatihan dengan himpunan data, lihat [Pelatihan dengan Himpunan Data](https://docs.microsoft.com/azure/machine-learning/how-to-train-with-datasets) di dokumentasi Azure Machine Learning."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
